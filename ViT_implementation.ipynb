{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "7fc88eb1-6f37-4812-b2ce-016b382fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "embed_size = 128\n",
    "batch_size = 32\n",
    "patch_height = 7\n",
    "number_of_patches = 16\n",
    "\n",
    "def patchify(images):\n",
    "    patched = torch.empty((images.size(dim=0), number_of_patches, patch_height*patch_height))\n",
    "    \n",
    "    for n in range(images.size(dim=0)):\n",
    "        for i in range(0,4):\n",
    "            for j in range(0,4):\n",
    "                patched[n,4*i+j] = torch.flatten( images[n,0,i*patch_height:(i+1)*patch_height, j*patch_height:(j+1)*patch_height] )\n",
    "    return patched\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numheads = 4\n",
    "        self.patches_with_class = number_of_patches + 1\n",
    "        \n",
    "        self.qlinear = nn.Linear(embed_size, embed_size)\n",
    "        self.klinear = nn.Linear(embed_size, embed_size)\n",
    "        self.vlinear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.headconcatlinear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self,x, verbose=False):\n",
    "        q = self.qlinear(x)\n",
    "        k = self.klinear(x)\n",
    "        v  =self.vlinear(x)\n",
    "        if verbose: print(\"q,v,k shape\", q.shape)\n",
    "        \n",
    "        # Create heads: torch.view to split last layer among heads. Torch.permute to place head number as first layer.\n",
    "        q = q.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        k = k.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        v = v.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        if verbose: print(\"After split the heads q, k,v shape:\", q.shape, \"\\n\")\n",
    "        \n",
    "        matmulqk = torch.matmul(q,torch.transpose(k, dim0=2, dim1=3)) / numpy.sqrt( int(embed_size / self.numheads) )\n",
    "        if verbose: print(\"multiplied key with query, shape:\", matmulqk.shape, \"\\n\")\n",
    "        \n",
    "        attention_weights = nn.Softmax(dim=-1)(matmulqk)\n",
    "        if verbose: print(\"attention weights after softmax: \\n\", attention_weights.shape)\n",
    "        \n",
    "        result = torch.matmul(attention_weights, v)\n",
    "        if verbose: print(\"Multiply attention weights and values shape:\", result.shape)\n",
    "        \n",
    "        result = result.permute(0,2,1,3)\n",
    "        if verbose: print(\"Bring heads together:\", result.shape, \"\\n\")\n",
    "        \n",
    "        result = result.reshape(batch_size, self.patches_with_class, embed_size)\n",
    "        if verbose: print(\"concatenate shape:\", result.shape, \" data:\\n\")\n",
    "        \n",
    "        result = self.headconcatlinear(result)\n",
    "        return result\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_normalization = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.ffblock = nn.Sequential(nn.Linear(embed_size, 2048), nn.ReLU(), nn.Linear(2048, embed_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        result = x + self.attention( self.layer_normalization(x) )  \n",
    "        result = result + self.ffblock( self.layer_normalization(result) )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "07515147-fdc1-43e1-a2a9-134433f383ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_size))\n",
    "        self.embedder = nn.Linear(patch_height*patch_height, embed_size)\n",
    "        \n",
    "        self.encoder1 = TransformerEncoder()\n",
    "        self.encoder2 = TransformerEncoder()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(nn.LayerNorm(embed_size), nn.Linear(embed_size, 10))\n",
    "    \n",
    "    def forward(self, x, verbose=False):        \n",
    "        \n",
    "        x = self.embedder(x)\n",
    "        cls_tokens = self.cls_token.repeat(batch_size,1,1)\n",
    "        x = torch.cat( (cls_tokens, x), dim=1 )\n",
    "        \n",
    "        x = PositionalEncoding(embed_size)(x)\n",
    "        \n",
    "        result = self.encoder1(x)\n",
    "        result = self.encoder2(result)\n",
    "        \n",
    "        result = result[:,0]\n",
    "        result = self.classification_head(result)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "113dfa8f-e60f-4d85-b985-ace13fbfa72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch {i}:\n",
      "\n",
      "2.4513885974884033\n",
      "1.8432921171188354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [328]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     12\u001b[0m     patches \u001b[38;5;241m=\u001b[39m patchify(x)\n\u001b[0;32m---> 13\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(yhat,y)\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [327]\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat( (cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m PositionalEncoding(embed_size)(x)\n\u001b[0;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2(result)\n\u001b[1;32m     24\u001b[0m result \u001b[38;5;241m=\u001b[39m result[:,\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [324]\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     91\u001b[0m     result \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_normalization(x) )  \n\u001b[0;32m---> 92\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffblock\u001b[49m( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_normalization(result) )\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1252\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root =\".\\data\", transform=transforms.ToTensor(), train=True, download=True )\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "model = ViT()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for i in numpy.arange(5):\n",
    "    print(\"Epoch {i}:\\n\")\n",
    "    for j, (x,y) in enumerate(dataloader):\n",
    "        patches = patchify(x)\n",
    "        yhat = model(patches)\n",
    "        \n",
    "        loss = criterion(yhat,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 100 == 0: \n",
    "            print(loss.item())\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "pyplot.plot(losses)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259cdfb-7cb4-4f05-bda2-5aca46ac0088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2381f79a-8537-4f79-8c43-b7f2d96ea9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e6f61-f343-414f-939a-3a512b46a463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
