{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc88eb1-6f37-4812-b2ce-016b382fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy, math\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "embed_size = 128\n",
    "batch_size = 32\n",
    "patch_height = 7\n",
    "number_of_patches = 16\n",
    "\n",
    "def patchify(images):\n",
    "    patched = torch.empty((images.size(dim=0), number_of_patches, patch_height*patch_height))\n",
    "    \n",
    "    for n in range(images.size(dim=0)):\n",
    "        for i in range(0,4):\n",
    "            for j in range(0,4):\n",
    "                patched[n,4*i+j] = torch.flatten( images[n,0,i*patch_height:(i+1)*patch_height, j*patch_height:(j+1)*patch_height] )\n",
    "    return patched\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numheads = 4\n",
    "        self.patches_with_class = number_of_patches + 1\n",
    "        \n",
    "        self.qlinear = nn.Linear(embed_size, embed_size)\n",
    "        self.klinear = nn.Linear(embed_size, embed_size)\n",
    "        self.vlinear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.headconcatlinear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self,x, verbose=False):\n",
    "        q = self.qlinear(x)\n",
    "        k = self.klinear(x)\n",
    "        v  =self.vlinear(x)\n",
    "        if verbose: print(\"q,v,k shape\", q.shape)\n",
    "        \n",
    "        # Create heads: torch.view to split last layer among heads. Torch.permute to place head number as first layer.\n",
    "        q = q.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        k = k.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        v = v.view(batch_size, self.patches_with_class, self.numheads, embed_size//self.numheads).permute(0,2,1,3)\n",
    "        if verbose: print(\"After split the heads q, k,v shape:\", q.shape, \"\\n\")\n",
    "        \n",
    "        matmulqk = torch.matmul(q,torch.transpose(k, dim0=2, dim1=3)) / numpy.sqrt( int(embed_size / self.numheads) )\n",
    "        if verbose: print(\"multiplied key with query, shape:\", matmulqk.shape, \"\\n\")\n",
    "        \n",
    "        attention_weights = nn.Softmax(dim=-1)(matmulqk)\n",
    "        if verbose: print(\"attention weights after softmax: \\n\", attention_weights.shape)\n",
    "        \n",
    "        result = torch.matmul(attention_weights, v)\n",
    "        if verbose: print(\"Multiply attention weights and values shape:\", result.shape)\n",
    "        \n",
    "        result = result.permute(0,2,1,3)\n",
    "        if verbose: print(\"Bring heads together:\", result.shape, \"\\n\")\n",
    "        \n",
    "        result = result.reshape(batch_size, self.patches_with_class, embed_size)\n",
    "        if verbose: print(\"concatenate shape:\", result.shape, \" data:\\n\")\n",
    "        \n",
    "        result = self.headconcatlinear(result)\n",
    "        return result\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_normalization = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.ffblock = nn.Sequential(nn.Linear(embed_size, 2048), nn.ReLU(), nn.Linear(2048, embed_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        result = x + self.attention( self.layer_normalization(x) )  \n",
    "        result = result + self.ffblock( self.layer_normalization(result) )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07515147-fdc1-43e1-a2a9-134433f383ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_size))\n",
    "        self.embedder = nn.Linear(patch_height*patch_height, embed_size)\n",
    "        \n",
    "        self.encoder1 = TransformerEncoder()\n",
    "        self.encoder2 = TransformerEncoder()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(nn.LayerNorm(embed_size), nn.Linear(embed_size, 10))\n",
    "    \n",
    "    def forward(self, x, verbose=False):        \n",
    "        \n",
    "        x = self.embedder(x)\n",
    "        cls_tokens = self.cls_token.repeat(batch_size,1,1)\n",
    "        x = torch.cat( (cls_tokens, x), dim=1 )\n",
    "        \n",
    "        x = PositionalEncoding(embed_size)(x)\n",
    "        \n",
    "        result = self.encoder1(x)\n",
    "        result = self.encoder2(result)\n",
    "        \n",
    "        result = result[:,0]\n",
    "        result = self.classification_head(result)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113dfa8f-e60f-4d85-b985-ace13fbfa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root =\".\\data\", transform=transforms.ToTensor(), train=True, download=True )\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "model = ViT()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for i in numpy.arange(5):\n",
    "    for j, (x,y) in enumerate(dataloader):\n",
    "        patches = patchify(x)\n",
    "        yhat = model(patches)\n",
    "        \n",
    "        loss = criterion(yhat,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 100 == 0: \n",
    "            print(loss.item())\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "pyplot.plot(losses)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
